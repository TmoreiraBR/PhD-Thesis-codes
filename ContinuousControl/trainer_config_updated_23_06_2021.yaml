behaviors:
  BehaviorPPO:
    trainer_type: ppo
    hyperparameters: # Hyperparameters common to PPO and SAC
      batch_size: 256
      buffer_size: 4056
      learning_rate: 1.5e-4
      # PPO-specific hyperparameters
      beta: 1.8e-2
      epsilon: 0.2
      lambd: 0.98
      num_epoch: 6
      learning_rate_schedule: linear
    network_settings: # Configuration of the neural network (common to PPO/SAC)
      normalize: true
      hidden_units: 48
      num_layers: 4
      vis_encoder_type: simple
      goal_conditioning_type: none
    reward_signals:
      extrinsic:
        gamma: 0.97
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 2
          num_layers: 2
          #vis_encode_type:  simple
           #memory: None
          #goal_conditioning_type: none
    # Trainer configurations common to all trainers
    keep_checkpoints: 5
    max_steps: 8.0e7
    time_horizon: 54
    summary_freq: 40000